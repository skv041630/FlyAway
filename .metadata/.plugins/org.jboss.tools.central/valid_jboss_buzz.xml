<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How debugging Go programs with Delve and eBPF is faster</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/13/how-debugging-go-programs-delve-and-ebpf-faster" /><author><name>Derek Parker</name></author><id>baa4a706-58dd-40c1-915c-e29f5b85965a</id><updated>2023-02-13T07:00:00Z</updated><published>2023-02-13T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I will explain how to use &lt;a href="https://github.com/go-delve/delve"&gt;Delve&lt;/a&gt; to trace your Go programs and how Delve leverages eBPF under the hood to maximize efficiency and speed. The goal of Delve is to provide developers with a pleasant and efficient Go debugging experience. In that vein, this article focuses on how we optimized the function tracing subsystem so you can inspect your programs and get to root-cause analysis quicker. Delve has two different backends for its tracing implementation, one is ptrace based, while the other uses eBPF. If you’re unfamiliar with any of these terms, don’t worry, I will explain along the way.&lt;/p&gt; &lt;h2&gt;What is program tracing?&lt;/h2&gt; &lt;p&gt;Tracing is a technique that allows a developer to see what the program is doing during execution. As opposed to typical debugging techniques, this method does not require direct user interaction. One of the most well-known tracing tools is &lt;a href="https://strace.io/"&gt;strace&lt;/a&gt;, which allows developers to see which system calls their program during execution.&lt;/p&gt; &lt;p&gt;While the aforementioned strace tool is useful for gaining insight into system calls, the Delve trace command allows you to gain insight into what is happening in "userspace" within your Go programs. This Delve trace technique allows you to trace arbitrary functions in your program in order to see the inputs and outputs of those functions. Additionally, you can also use this tool to gain insight into the control flow of your program without the overhead of an interactive debugging session as it will also display with Goroutine is executing the function. For highly concurrent programs this can be a quicker way to gain insights into your programs execution without starting a full interactive debugging session.&lt;/p&gt; &lt;h2&gt;How to trace Go programs with Delve&lt;/h2&gt; &lt;p&gt;Delve allows you to trace your Go programs by invoking the &lt;code&gt;dlv trace&lt;/code&gt; subcommand. The subcommand accepts a regular expression and will execute your program, setting a tracepoint on each function that matches the regular expression and displaying the results in real time.&lt;/p&gt; &lt;p&gt;The following program is an example:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;package main import "fmt" func foo(x, y int) (z int) {         fmt.Printf("x=%d, y=%d, z=%d\n", x, y, z)         z = x + y         return } func main() {         x := 99         y := x * x         z := foo(x, y)         fmt.Printf("z=%d\n", z) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tracing this program will give you the following output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ dlv trace foo &gt; goroutine(1): main.foo(99, 9801) x=99, y=9801, z=0 &gt;&gt; goroutine(1): =&gt; (9900) z=9900 Process 583475 has exited with status 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, we supplied &lt;code&gt;foo&lt;/code&gt; as the regexp, which in this case, matched the function of the same name in the main package. The output prefixed with &lt;code&gt;&gt;&lt;/code&gt; denotes the function being called and shows the arguments the function was called by, while the output prefixed with &lt;code&gt;&gt;&gt;&lt;/code&gt; denotes the return from the function and the return value associated with it. All input and output lines are prefixed with the Goroutine executing at the time.&lt;/p&gt; &lt;p&gt;By default, the &lt;code&gt;dlv trace&lt;/code&gt; command uses the ptrace based backend, however adding the &lt;code&gt;--ebpf&lt;/code&gt; flag will enable the experimental eBPF based backend. Using the previous example, if we were to invoke the trace subcommand like the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ dlv trace –ebpf foo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We would receive similar output. However, what happens behind the scenes is much different and significantly more efficient.&lt;/p&gt; &lt;h2&gt;The inefficiencies of ptrace&lt;/h2&gt; &lt;p&gt;By default, Delve will use the ptrace syscall in order to implement the tracing feature. The &lt;a href="https://man7.org/linux/man-pages/man2/ptrace.2.html"&gt;ptrace&lt;/a&gt; is a syscall that allows programs to observe and manipulate other programs on the same machine. In fact, on Unix systems, Delve uses this ptrace functionality to implement many low-level functionalities provided by the debugger, such as reading/writing memory, controlling execution, and more.&lt;/p&gt; &lt;p&gt;While ptrace is a useful and powerful mechanism, it suffers from inherent inefficiencies. First, the fact that ptrace is a syscall means that we must cross the user space/kernel space boundary, which adds overhead every time the function is used. This is compounded by the number of times we have to invoke ptrace in order to achieve the desired results. Considering the previous example, the following is a rough outline of the tracing implementation steps using ptrace:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start the program and attach the debugger using `ptrace(PT_ATTACH)`.&lt;/li&gt; &lt;li&gt;Set a breakpoint at each function which matches the provided regular expression, using `ptrace` to insert the breakpoint instruction into the traced processes executable memory.&lt;/li&gt; &lt;li&gt;Additionally, set breakpoint at each return instruction for that function.&lt;/li&gt; &lt;li&gt;Continue the program, again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit breakpoint at function entry, and read function arguments. This step can involve many ptrace calls as we read CPU registers, memory on the stack and memory in the heap if we must dereference a pointer.&lt;/li&gt; &lt;li&gt;Continue the program again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit breakpoint at function return, going through the same aforementioned process to read variables potentially involving many more calls to `ptrace` to read registers and memory.&lt;/li&gt; &lt;li&gt;Continue the program again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Repeat until the program ends.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Obviously, the more arguments and return values the function has, the more expensive every stop becomes. All the time the debugger spends making these `ptrace` syscalls, the program we are tracing is paused and not executing any instructions. From the users’ perspective, this makes the program run significantly slower than it otherwise would. Now, for development and debugging, maybe this isn’t such a big deal, but time is precious, and we should endeavor to do things as quickly as possible. The quicker your program runs while tracing, the quicker you can get to the root cause of the issue you’re trying to debug.&lt;/p&gt; &lt;p&gt;Now, the question becomes, how can we make this better? In the next section, we discuss the new eBPF based backend and how it improves upon this approach.&lt;/p&gt; &lt;h2&gt;How eBPF is faster than ptrace&lt;/h2&gt; &lt;p&gt;One of the biggest speed and efficiency improvements we can make is to avoid a lot of the syscall overhead altogether. This is where &lt;a href="https://ebpf.io/"&gt;eBPF&lt;/a&gt; comes into play because instead of setting breakpoints on each function, we can instead set uprobes on function entry and exit and attach small &lt;a href="https://github.com/go-delve/delve/blob/master/pkg/proc/internal/ebpf/bpf/trace.bpf.c"&gt;eBPF programs&lt;/a&gt; to them. Delve uses the &lt;a href="https://github.com/cilium/ebpf"&gt;Cilium eBPF&lt;/a&gt; Go library to &lt;a href="https://github.com/go-delve/delve/blob/master/pkg/proc/internal/ebpf/helpers.go#L105"&gt;load and interact&lt;/a&gt; with the eBPF programs.&lt;/p&gt; &lt;p&gt;Each time the probe is hit, the kernel will invoke our eBPF program and then continue the main program once it has completed. The small eBPF program we write will handle all of the steps listed above at function entry and exit but without all the syscall context switching because the program executes directly within kernel space. Our eBPF program is able to communicate with the debugger in userspace via eBPF ringbuffer and map data structures, allowing Delve to collect all of the information it needs.&lt;/p&gt; &lt;p&gt;The benefit of this approach is that the time the program we are tracing needs to be paused is significantly decreased. Running our eBPF program when a probe is hit is much quicker than invoking multiple syscalls at function entry and exit.&lt;/p&gt; &lt;h2&gt;The flow of tracing and debugging using eBPF&lt;/h2&gt; &lt;p&gt;Again, using the previous example, the following is a rough outline of the tracing implementation steps using eBPF:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start the program and attach using `ptrace(PT_ATTACH)`.&lt;/li&gt; &lt;li&gt;Load all uprobes into the kernel for each function to trace.&lt;/li&gt; &lt;li&gt;Continue the program using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit uprobes at function entry / exit. In kernel space, each time a probe is hit, the kernel runs our eBPF program, which gathers function arguments or return values and sends them back to userspace. In user space, read from eBPF ringbuffer as function arguments, and return values are sent.&lt;/li&gt; &lt;li&gt;Repeat until the program ends.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Using this method, Delve is able to trace a program in significantly less time than with the default ptrace implementation. Now, you may ask, if it is so much more efficient to use this method, why not make it the default? Eventually, it likely will be made default. But for the time being, development is still ongoing to improve this eBPF based backend and ensure it has parity with the ptrace based one. However, you can still use it today by supplying the `--ebpf` flag during the `dlv trace` invocation.&lt;/p&gt; &lt;p&gt;To give a sense of how much more efficient this method is, I measured a different example program running by itself and then under the different tracing methods with the following results.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Program execution: 23.7µs With eBPF trace: 683.1µs With ptrace tracing: 2.3s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The numbers speak for themselves!&lt;/p&gt; &lt;h2&gt;Why not use uretprobes?&lt;/h2&gt; &lt;p&gt;If you're familiar with eBPF a uprobes / uretprobes you may be asking yourself why we use uprobes for everything as opposed to simply using uretprobes to capture return arguments. The explanation for this gets relatively complex, but the short version is that the Go runtime needs to inspect the call stack at various times during the execution of a Go program. When uretprobes are attached to a function they overwrite the return address of that function on the stack. When the Go runtime then inspects the stack it finds an unexpected return address for the function and will end up fatally exiting the program. To work around this we simply use uprobes and leveraging Delves ability to inspect the machine instructions of the program to set probes at each return instruction for a function.&lt;/p&gt; &lt;h2&gt;Delve debugs Go code faster with eBPF&lt;/h2&gt; &lt;p&gt;The overall goal of Delve is to help developers find bugs in their Go code as quickly as possible. To do this, we leverage the latest methods and tech available and try to push the boundaries of what a debugger can accomplish. Delve leverages eBPF under the hood to maximize efficiency and speed. User space tracing is a great tool for any engineer to have in their toolbox, and we aim to make it efficient and easy to use.&lt;/p&gt; &lt;p&gt;Building and delivering modern, innovative apps and services is more complicated and fast-moving than ever. Join the Red Hat Developer program for tools, technologies, and community to level up your knowledge and career. &lt;a href="https://developers.redhat.com/about"&gt;Learn more...&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/13/how-debugging-go-programs-delve-and-ebpf-faster" title="How debugging Go programs with Delve and eBPF is faster"&gt;How debugging Go programs with Delve and eBPF is faster&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Derek Parker</dc:creator><dc:date>2023-02-13T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.33.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/02/kogito-1-33-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/02/kogito-1-33-0-released.html</id><updated>2023-02-13T00:58:30Z</updated><content type="html">We are glad to announce that the Kogito 1.33.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Quarkus 2.16 integration * Infinispan upgrade to version 14.0.4 * Keycloack upgrade to version 20.0.2 * New Data Index addons that allow running indexing capabilities as part of Kogito runtimes. For more details, visit the complete .  This includes the following new addons as Quarkus extensions: * kogito-addons-quarkus-data-index-infinispan * kogito-addons-quarkus-data-index-mongodb * kogito-addons-quarkus-data-index-inmemory * Kogito-addons-quarkus-data-index-postgresql * Integration with AsyncAPI quarkiverse extension * Flyway added to help migrate Data Index schema based on Oracle database with Kogito upgrade. * Serverless Workflow can now integrate with Camel Routes defined within the same Maven project. See this blog post for more information.  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.26.0 artifacts are available at the . A detailed changelog for 1.33.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Serverless Workflow integration with Camel Routes</title><link rel="alternate" href="https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html</id><updated>2023-02-10T17:58:06Z</updated><content type="html">A recent addition to the Kogito Serverless Workflow is the ability to call ! WHEN TO USE CAMEL ROUTES that provides to integrate with virtually any technology stack. Kogito Serverless Workflow offers a few ways to make calls to remote services that expose standard interfaces such as OpenAPI and AsyncAPI. Sometimes, you might need to make calls to legacy systems or specific services that require additional configuration, schema, or data structures so that the standard interfaces might not suffice. For example, you might need to call a from the workflow. Without additional Java implementation in your Kogito workflow application, you won’t be able to make requests to this SOAP service directly. What if you could explicitly declare in your workflow definition that your function is a call to a Camel route in your project context? HOW THE INTEGRATION WORKS Kogito Serverless Workflow has a new add-on capable of producing messages to a Camel Route within the same application instance. For example, you have a Camel route in your project that can interface with a SOAP WebService like this one: &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!-- might look like that we are using Spring, but we are not. The XML DSL is derived from there, though --&gt; &lt;routes xmlns="http://camel.apache.org/schema/spring"&gt; &lt;route id="sendSoapMessage"&gt; &lt;from uri="direct:numberToWords"/&gt; &lt;!-- Converting our body to the type expected by the SOAP interface --&gt; &lt;bean beanType="java.math.BigInteger" method="valueOf"/&gt; &lt;setHeader name="operationName"&gt; &lt;constant&gt;NumberToWords&lt;/constant&gt; &lt;/setHeader&gt; &lt;toD uri="cxf://example.com?serviceClass=com.dataaccess.webservicesserver.NumberConversionSoapType&amp;#38;wsdlURL=/wsdl/numberconversion.wsdl"/&gt; &lt;/route&gt; &lt;/routes&gt; Then, from the workflow definition, you can declare a function that produces messages to this route: { "functions": [ { "name": "callSoap", "type": "custom", "operation": "camel:direct:numberToWords" } ] } Notice the new custom function with a new operation type. The operation is a URI scheme composed of the constant "camel:", the "direct:" endpoint, and its name. Kogito Serverless Workflow only supports producing messages to a endpoint at this time. To use this function in the State definition, you can refer to the function as you usually would: { "states": [ { "name": "start", "type": "operation", "actions": [ { "functionRef": { "refName": "callSoap", "arguments": { "body": "${ .number }", "headers": { "header1": "value1", "header2": "value2"} } } } ], "end": true } ]} The function arguments can have optional attributes, "body" and "headers." These arguments will be constructed as part of the handled internally by the Kogito engine. The body can be any valid JSON object and the headers must be a key/value pair. Your route is responsible for properly handling the message. In the example above, the body is the number contained in the JSON payload. The route response must be a valid Java bean object that can be serialized to JSON or a primitive type. Note that a JSON string is a valid output. The data will be merged into the workflow context data in the response attribute, for example: { "fruit": "orange", "response": { "number": 10 } } , you will find more information about this scenario and the complete project example. FINAL THOUGHTS The Camel Kogito add-on complements our work of integrating with the. In the use case described in this post, the Camel route is tightly coupled to the workflow. If you need to reuse the route or have more complex interface interactions (such as a REST endpoint), you should use Camel-K and interact with the services via OpenAPI interfaces. Overall, this new feature introduced by Kogito Serverless Workflow can solve many use cases and enable integration with any service interface or data format supported by Camel. It’s a new way of interacting with services that don’t have standard interfaces or formats available, all within the same application. In a world where many companies are looking to modernize their architecture and lift to the cloud, we believe this new feature can help them through the journey. Leave a comment or open a thread on our if you have any questions. The post appeared first on .</content><dc:creator>Ricardo Zanini</dc:creator></entry><entry><title>How to run Camel on Spring Boot in Red Hat Developer Sandbox</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/10/how-run-camel-spring-boot-red-hat-developer-sandbox" /><author><name>Bruno Meseguer</name></author><id>79312141-32b5-4731-b96d-7953fa890b34</id><updated>2023-02-10T07:00:00Z</updated><published>2023-02-10T07:00:00Z</published><summary type="html">&lt;p&gt;Apache Camel is known as the Swiss knife of integration. Yet many developers building &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and delivering platforms that connect apps with backends are still unaware of integration frameworks like Camel, which has been perfected over the years to provide efficient and economical building blocks. Apache Camel excels at simplifying and standardizing your landscape of platform integration services. Camel on &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; is one of the various options for running Apache Camel. Although the latest evolution of Camel offers new runtimes, such as &lt;a href="https://developers.redhat.com/node/219015"&gt;Quarkus&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt; (also Quarkus-based), Spring Boot is still a popular option for development teams with strong &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; know-how.&lt;/p&gt; &lt;p&gt;To learn more about the various Camel runtimes available, read the 3-part series article, &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;Choose the best Camel for your integration ride&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Why you should try Camel in the Developer Sandbox&lt;/h2&gt; &lt;p&gt;Experience Camel in the &lt;a href="https://red.ht/sandb0x"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, a hands-on, free environment. In this developer environment, you can play with Camel Spring Boot and try many other technologies.&lt;/p&gt; &lt;p&gt;You’ll be able to do the following without the hassle of setting up your local environment:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Play for free in an OpenShift environment (Developer Sandbox).&lt;/li&gt; &lt;li aria-level="1"&gt;Use the web-based IDE (&lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt;, formerly Red Hat CodeReady Workspaces) to sample a demo Camel on the Spring Boot project.&lt;/li&gt; &lt;li aria-level="1"&gt;Run the Camel demo application as if you were developing it in your own IDE.&lt;/li&gt; &lt;li aria-level="1"&gt;Deploy your Camel application in the Developer Sandbox.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;A demonstration of running Camel on Spring Boot&lt;/h2&gt; &lt;p&gt;This demo includes an OpenAPI service called &lt;strong&gt;simple&lt;/strong&gt;, including a stub serving XML data to simulate the backend service Camel integrates with. It’s a relatively simple use case but very common in the enterprise. It defines a REST API and hides a legacy service behind the scenes (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bruno-1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bruno-1.png?itok=rLNFTVid" width="600" height="338" alt="A diagram of the flow of Camel on Spring Boot." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Camel on Spring Boot flow.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This example stands out from the data handling point of view, where the JSON input needs to be mapped to the outgoing XML during the request flow and performs the reverse format translation during the response flow, as in Figure 1.&lt;/p&gt; &lt;p&gt;What is unique about Camel running on Spring Boot, but also true for all runtimes, is how little code is required and how elegantly it is laid out. This simplicity guarantees economical, long-term, and sustainable support for your landscape of implemented Camel services.&lt;/p&gt; &lt;p&gt;If you would like to see how all of this is done, jump straight onto the Developer Sandbox to explore the code and execute it. The next section demonstrates how to access the Developer Sandbox. &lt;/p&gt; &lt;h3&gt;How to access the Developer Sandbox&lt;/h3&gt; &lt;p&gt;First things first, &lt;a href="developers.redhat.com/developer-sandbox"&gt;go to the Developer Sandbox&lt;/a&gt; to learn about what you can do. &lt;span&gt;Then follow these steps:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;span&gt;Click on the &lt;strong&gt;Start your sandbox for free&lt;/strong&gt; button, as shown in the top image in Figure 2.&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bruno-2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bruno-2.png?itok=htqmf95h" width="600" height="338" alt="Two screenshots of the Sandbox start and Red Hat login pages." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Logging into the Sandbox and Red Hat account.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;After which, you will see a login window shown in the bottom image in Figure 2. &lt;/li&gt; &lt;li&gt;If you don’t have a Red Hat account, click on &lt;strong&gt;Register for a Red Hat account&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Enter the details requested and complete the registration process.&lt;/li&gt; &lt;li&gt;Once you successfully log in, you will be presented with an option in Figure 3. &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bruno-3.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bruno-3.jpg?itok=YULQdu7E" width="600" height="276" alt="Two screenshots of the Developer Sandbox start and login pages." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Enter the Sandbox.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt; &lt;/p&gt; &lt;ul&gt;&lt;li&gt;Click on the red button, &lt;strong&gt;Start your sandbox for free&lt;/strong&gt;. This action will take you to the OpenShift dedicated login page.&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;DevSandbox&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Once you’re in, the environment will welcome you with the message, &lt;strong&gt;Welcome to the Developer Perspective!&lt;/strong&gt; Feel free to start with a tour that explains the interface features.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Developer sandbox ships with an entire web-based IDE entitled OpenShift Dev Spaces from which you can play with Camel on Spring Boot in a typical developer workflow using a code editor and a terminal.&lt;/p&gt; &lt;h3&gt;How to prepare your dev environment&lt;/h3&gt; &lt;p&gt;To open Dev Spaces: &lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the applications icon as shown in Figure 4.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Red Hat OpenShift Dev Spaces&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bruno-4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bruno-4.png?itok=j48SIWUW" width="600" height="287" alt="Two screenshots of the Create Workspace dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Create a Workspace.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;When the &lt;strong&gt;Create Workspace dashboard&lt;/strong&gt; in Dev Spaces opens, enter the Git Repo URL in the highlighted text box in the bottom picture (Figure 4): https://github.com/RedHat-Middleware-Workshops/devsandbox-camel.gi.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create &amp; Open&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;While the workspace provisions, you should see a progress log similar to Figure 5.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/18-dev-spaces-provision-workspace.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/18-dev-spaces-provision-workspace.png?itok=L5wSPVJQ" width="600" height="365" alt="A progress log for starting a workspace." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Starting a Workspace progress log.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It will take some time to provision the workspace instance. When finished, it will present you with the IDE environment. &lt;/p&gt; &lt;p&gt;When the IDE shows up, you will open the tutorial that will guide you through the Camel demo project.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Click the deployable &lt;strong&gt;Endpoints&lt;/strong&gt; accordion marked 1 (Figure 6).&lt;/li&gt; &lt;li&gt;Click on the &lt;strong&gt;tutorial&lt;/strong&gt; (marked 2) which opens in a new browser tab. &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/19-dev-spaces-open-tutorial.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/19-dev-spaces-open-tutorial.png?itok=WAkzCzcH" width="568" height="976" alt="A screenshot of the menu showing endpoints and tutorial options." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Open the tutorial for the Camel demo project.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt; &lt;/p&gt; &lt;ul&gt;&lt;li&gt;You will be prompted to confirm you want to open an external website, so click &lt;strong&gt;Open&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The new browser tab will display the Solution Explorer interface containing tiles for different labs (Figure 7). Select the highlighted tile, &lt;strong&gt;Camel Spring Boot - Simple&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/20-solution-explorer.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/20-solution-explorer.png?itok=IUFNAxmU" width="600" height="395" alt="The Solution Explorer interface showing lab tiles." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 7: The Solution Explorer interface showing the Camel Spring Boot-Simple lab.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you click on the tile, the Solution Explorer will show the lab introduction and the exercise chapters, which you can complete in around 15 minutes. The exercises involve easy copy/paste actions to provide a smooth experience, demonstrating the highlights of Camel Spring Boot.&lt;/p&gt; &lt;p&gt;Enjoy the Camel&lt;em&gt; &lt;/em&gt; ride!&lt;/p&gt; &lt;h2&gt;This is only the start&lt;/h2&gt; &lt;p&gt;This article ends here, but this should only be the start of your journey with Apache Camel. The Developer Sandbox gives you the opportunity to play on a Kubernetes-based application platform with an integrated developer IDE (OpenShift Dev Spaces). Even for those unfamiliar with Kubernetes environments, this demo lab is great for sampling what it’s like to build applications with Camel.&lt;/p&gt; &lt;p&gt;With just your browser, you can quickly complete the Camel Spring Boot lab and see for yourself how simply Camel resolves a typical use case and how easy it is to test, containerize, and run in OpenShift. Camel on Spring Boot keeps the key benefits Spring Boot lovers seek with easier development, auto-configuration, straightforward setup, and management.&lt;/p&gt; &lt;p&gt;Camel has also evolved to run in container-tailored runtimes specifically designed to minimize memory footprint and maximize performance. If you want to learn more, check out these resources:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Learn more about the different Camel&lt;em&gt; &lt;/em&gt; runtimes available by reading &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;Choose the best Camel for your integration ride&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Explore Camel Quarkus in more detail by reading&lt;a href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus"&gt; Boost Apache Camel performance with Quarkus&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Red Hat Developer's &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K topic page&lt;/a&gt; is a good place to start learning about Camel K.&lt;/li&gt; &lt;li aria-level="1"&gt;Visit &lt;a href="https://developers.redhat.com/products/integration/overview"&gt; Red Hat Integration&lt;/a&gt; page to discover Camel complementary capabilities.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/10/how-run-camel-spring-boot-red-hat-developer-sandbox" title="How to run Camel on Spring Boot in Red Hat Developer Sandbox"&gt;How to run Camel on Spring Boot in Red Hat Developer Sandbox&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bruno Meseguer</dc:creator><dc:date>2023-02-10T07:00:00Z</dc:date></entry><entry><title>Express Hibernate Queries as Type-Safe Java Streams</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/jpastreamer-extension/&#xA;            " /><author><name>Julia Gustafsson (https://twitter.com/)</name></author><id>https://quarkus.io/blog/jpastreamer-extension/</id><updated>2023-02-10T00:00:00Z</updated><published>2023-02-10T00:00:00Z</published><summary type="html">Writing Hibernate queries using the Criteria API can be anything but intuitive and comes at the expense of wordiness. In this article, you will learn how the JPAStreamer Quarkus extension facilitates type-safe Hibernate queries without unnecessary complexity. As much as the JPA Criteria builder is expressive, JPA queries are often...</summary><dc:creator>Julia Gustafsson (https://twitter.com/)</dc:creator><dc:date>2023-02-10T00:00:00Z</dc:date></entry><entry><title>DataCater uses Quarkus to make Data Streaming more accessible</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/&#xA;            " /><author><name>Stefan Sprenger (https://twitter.com/flipping_bits)</name></author><id>https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/</id><updated>2023-02-09T00:00:00Z</updated><published>2023-02-09T00:00:00Z</published><summary type="html">This article gives a brief overview of the data streaming platform DataCater, discusses how we moved from Scala Play! and Kafka Streams to Quarkus, and presents why we think that Quarkus is an exceptional framework for developing cloud-native Java applications. What is DataCater? DataCater is a real-time, cloud-native data pipeline...</summary><dc:creator>Stefan Sprenger (https://twitter.com/flipping_bits)</dc:creator><dc:date>2023-02-09T00:00:00Z</dc:date></entry><entry><title type="html">Synchronize your KIE Sandbox workspace with Bitbucket or GitHub</title><link rel="alternate" href="https://blog.kie.org/2023/02/synchronize-your-kie-sandbox-workspace-with-bitbucket-or-github.html" /><author><name>Jan Stastny</name></author><id>https://blog.kie.org/2023/02/synchronize-your-kie-sandbox-workspace-with-bitbucket-or-github.html</id><updated>2023-02-08T18:31:37Z</updated><content type="html">1 INTRODUCTION  KIE Sandbox now brings the possibility to synchronize your changes not only with GitHub, but also Bitbucket.  &gt; Bitbucket has several variants of their offering. Only support for Bitbucket &gt; Cloud is available, namely bitbucket.org – a publicly hosted instance. Other &gt; Bitbucket variants, being it Bitbucket Server or Bitbucket Data Center are not &gt; supported, due to significant differences in provided APIs. Support for enterprise instances of Bitbucket Cloud will arrive in future.  2 CONNECTING TO AN ACCOUNT  AUTHORIZATION  For KIE Sandbox to be able to communicate with a given git instance, it needs to be provided a way to authenticate with. In the case of Bitbucket, the supported way is to use a so-called App password, in the case of GitHub, its name is Personal Access Token. They are long-lasting OAuth2 tokens with selected scopes that enable the actions in KIE Sandbox when working with our workspace.  List of related OAuth2 scopes:  -------------------------------------------------------------------------------- Bitbucket scopeDescriptionFor the initial authentication and user details retrieval (e.g., uuid of the user)  Read access to repositories of given user.Write access to all the repositories the authorizing user has access to. To create repositories Read access to all the snippets the authorizing user has access to.  Write access to all the snippets the authorizing user can edit Required Bitbucket OAuth2 scopes () -------------------------------------------------------------------------------- GitHub scopeDescription(no-scope)Implicit read access to public repositories and Gists. repoFull access to public and private repositories.gistGrants write access to Gists.  Required GitHub OAuth2 scopes () -------------------------------------------------------------------------------- KIE SANDBOX CONNECTED ACCOUNTS  Once we have a token, we need to configure the KIE Sandbox to make it available in there.   You can configure the authentication in advance by visiting the Connected Accounts section in the top panel. KIE Sandbox serves as a modal where the user needs to specify required values. In the case of Bitbucket, it is a username and the app password OAuth2 token. This is a difference compared to GitHub, where the token itself carries the user information and is sufficient for authentication on its own.  After the addition, you should see the account information together with usage statistics in your current instance of KIE Sandbox.   Once you connect the account, it’s available in your workspaces through the Authentication source section in the Share dropdown. Where you can also connect to another account by using the respective link from the dropdown, which serves as a shortcut to the Connected Accounts section mentioned earlier.  3 COLLABORATION IS KEY (KIE)  It’s always better to share. And with KIE Sandbox sharing is as easy as a single click to synchronize your changes with the remote location.  &gt; KIE Sandbox does not aspire at replacing the interaction with the existing git &gt; tools or git vendor UI, rather it simplifies most common operations vital for &gt; the smooth experience when modelling your assets.  COLLABORATE OVER AN EXISTING REPOSITORY  The ability to import projects and later synchronize your local changes back into the original location makes KIE Sandbox collaborative experience a breeze.  IMPORT FROM URL  It all starts with a URL. You can import existing projects on KIE Sandbox main screen by pasting their respective URL into the From URL widget. Once it’s confirmed as supported, the widget itself queries git-related information to be used and presents it to the user allowing to override the identified defaults, e.g., to change a branch to be checked out (click Change… if you want to do so).  After the project is imported, you’re taken into its workspace which is readily configured with the respective Authentication source, so you’re ready to Push or Pull without any further action.  SUPPORTED IMPORT URL OPTIONS  HostnameURL Contextbitbucket.org /:workspace/:repo   /:workspace/:repo/src/:tree   /:workspace/:repo/src/:tree/:path  /:workspace/workspace/snippets/:snippet_id/:snippet_name  /:workspace/workspace/snippets/:snippet_id/:snippet_name#file-:path  /snippets/:workspace/:snippet_id/:snippet_name.git github.com /:org/:repo  /:org/:repo/tree/:tree  /:org/:repo/tree/:tree/:path* raw.githubusercontent.com/:org/:repo/:tree/:path* gist.github.com or gist.githubusercontent.com /:user/:gistId/  /:user/:gistId/raw/:fileId/:fileName  /:gistId  SHARING LOCAL PROJECTS WITH THE WORLD  Having created a complex project including several models, now you wonder how to get those to your colleagues? No need to download and send by email, just create a brand-new git repository based on your workspace contents. By picking a different Authentication source, you can select between all your connected git vendor accounts.  PICK AN AUTHENTICATION SOURCE  The first step is to decide which Authentication source we’d like to use to share our workspace. Choice defines the actions allowed with the workspace further on.  Editor toolbar reflects this decision in its Share dropdown, where only actions applicable to your setup are available upon Authentication source selection.    Once you pick any of these options, your project is transformed into git-based workspace. Though there are some differences in how you can synchronize the contents, see them described below.  CREATE A GIT REPOSITORY  Create a repository by clicking on the respective button provided in the Share dropdown. Afterwards a modal appears where you can specify an intended location, repository name, and its visibility.  Location must be specified using a dynamically populated select field. The list of options is possible to be reload using the sync icon to its right.  In the case of GitHub, we can choose between creating the repository under our user location and creating in a GitHub organization that we are members of.   In the case of Bitbucket we select from either personal or shared workspaces. The repository name text field is populated with the value matching your workspace name, so if you’ve configured your workspace thoroughly, you should be all set. Though you are free to specify the name that suits your needs, with just a few limitations on the characters allowed guarded by validation. The Public and Private checkboxes define the repository visibility.  After the repository is created, you’re taken back into workspace, which is readily configured with the respective Authentication source, so you’re ready to Push or Pull without any further action.  CREATE A BITBUCKET SNIPPET OR GITHUB GIST  Do you hesitate to create a separate repository for something you were just doodling? Then share it just as a Bitbucket Snippet or GitHub Gist. Both these options provide you with the ability to push or pull changes, share by URL, etc.  After clicking the respective button, a new Modal dialog is displayed. We must specify  * a location under which the item will be created.  * In the case of GitHub Gist, the only option is the user account itself, it is not supported by GitHub to create a Gist in an organization. The select field is thus displayed as disabled.  * On the other hand, Bitbucket allows us to create Snippets generally in any workspace, as long as we have necessary permissions.  * Visibility of the item being created.  After confirming the dialog, the modal stays displayed until the operation is completed. After that notification alert with the newly created item is displayed. When the creation succeeds, you’re taken back into the workspace, which is now configured accordingly.  In comparison to the regular git repository Sync options above, you now have just a single operation listed there – update.  This action allows you to promote the changes back into the original location.  There is no choice to pull, i.e., update your workspace based on changes done either in Gist or Snippet. If you need a more flexible approach, let’s head for the following section.  TURN GIST OR SNIPPET INTO A REGULAR GIT REPOSITORY  Working with a Snippet or Gist and you’re finally satisfied with the models? Or do you want a more flexible workflow requiring updating your workspace with remote changes?  Turn your project into a regular git repository – the Share dropdown option is still there for Gist or Snippet–based workspaces. You’re not even limited to the same Authentication source here, e.g., for a GitHub Gist based workspace, you can easily share it as a Bitbucket repository afterwards, given that you have both accounts connected.  SUMMARY  * ✅We’ve seen how users can integrate KIE Sandbox into their git-based workflow.  * 💥KIE Sandbox now supports GitHub and Bitbucket Cloud.  * 💥Toolbar buttons now reflect the currently selected Authentication provider.  * 💥GitHub Repositories can also be created in GitHub organizations.  The post appeared first on .</content><dc:creator>Jan Stastny</dc:creator></entry><entry><title type="html">CloudEvents labeling and classification with Drools</title><link rel="alternate" href="https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html</id><updated>2023-02-08T15:40:00Z</updated><content type="html">This blog post is a quick update on a demo in labeling CNCF’s CloudEvents. INTRODUCTION Categorizing events is a general, common use-case; in the context of this post, we will delve into labeling CNCF’s CloudEvents for Intelligent Response Management (IRM) which can find application in several ways. One way is to categorize and prioritize different types of events based on their urgency or importance; for example: a SRE team might label an event as "critical" if it involves a major service outage, or "low priority" if it is a minor issue of a sub-system that can be resolved at a later time. This allows the team to quickly respond to the most pressing issues and allocate resources accordingly. Additionally, labeling events can also be used to track and analyze patterns in a system (or cluster) behaviors, which can help to identify potential problems before they occur and improve the overall reliability of the system by implementing corrective actions preventively. This demo make use of several technologies: * and YaRD for the rule definition and evaluation * for cloud native based decisioning * for cloud native Java development on top of Kubernetes * –this is the format of the event data that we want to process * Kafka as an event broker * PostgreSQL as our data store; we’re using specifically PostgreSQL for very interesting query capabilities that this RDBMS can offer * to define our custom types on top of PostgreSQL * extension * for the web-based GUI ARCHITECTURE The following is a high level diagram of the overall architecture for this demo: On the left hand side, the incoming CloudEvents instance that we want to process by labeling, is received by the endpoint which represents one of the possible inputs for this application. The CloudEvents instance is then immediately placed on a kafka topic, which is used to better isolate the ingress portion of this application from the rest of the processing pipeline. The processing pipeline starts with a labeling processor: this is the component responsible for applying the rules to enrich the CloudEvents instance with the required and applicable labels. As a result, the received message is now enriched with labeling and it gets persisted inside the data store. PostgreSQL is used specifically here as it provides hierarchical labels via ltree data type and related query capabilities, which are very useful in categorization applications such as this one. These advanced query capabilities are also foundational to potentially re-process the same CloudEvents instance, after some further augmentation or additional manual labeling. In the context of this article, the web-based GUI is provisional and will be used only as a practical demonstrator for the rich query capabilities. WALKTHROUGH A CloudEvents instance is submitted to this application, for example: { "specversion": "1.0", "id": "matteo-8eb9-43b2-9313-22133f2c747a", "source": "example", "type": "demo20220715contextlabel.demotype", "data": { "host": "basedidati.milano.local", "diskPerc": 70, "memPerc": 50, "cpuPerc": 20 } } The data context of the CloudEvents instance pertains to some host which came under supervision due to resource load. We now want to classify this context/case, using some labels. We may have more than one label. Each label is hierarchical (root.branch1.branch2.leaf). We want to classify the hostname based on its relevance to the department, unit, person or team responsible for it. To do so, a simple decision table provides an easy solution. For example, we can classify the hostname based on geographical location or determine the type of server based on the hostname. Ultimately, we might want to setup a labeling rule for who’s on call, something like the following decision table using : type: DecisionTable inputs: ['.location', '.type'] rules: - when: ['startswith("location.emea")', '. == "type.db"'] then: '"oncall.EMEA.dbadm"' - when: ['startswith("location.emea") | not', '. == "type.db"'] then: '"oncall.CORP.dbadm"' - when: ['true', '. == "type.nas"'] then: '"oncall.CORP.it"' For example, a CloudEvents context may be labeled as follows: * type.db * location.emea.italy.milan * oncall.EMEA.dbadm For the PostgreSQL DDL we currently have: Table "public.cecase" Column | Type | Collation | Nullable | Default ---------+------------------------+-----------+----------+--------- id | bigint | | not null | ceuuid | character varying(255) | | | context | jsonb | | | mytag | ltree[] | | | Indexes: "cecase_pkey" PRIMARY KEY, btree (id) "mytag_gist_idx" gist (mytag) "mytag_idx" btree (mytag) Please notice we’re taking advantage here of PostgreSQL’s jsonb for storing the original CloudEvents context, and ltree[] data type for searching ad-hoc with indexing the hierarchical labels. The latter is extremely helpful also to setup queries making use of and ~ operators for PostgreSQL which performs on the ltree data type, showcased below. As the data flows into the application, we can use the provisional web-based GUI which provide a convenient way to consume the backend REST API(s) developed on Quarkus: In the screenshot above, you can access all the records from the table, where the labels have been applied by the rule definition. We can browse by having at least one label having the specified parent, with a query like: SELECT * FROM cecase WHERE mytag &lt;@ 'oncall.CORP' For example, if we want all the records having at least a label for the oncall.CORP rooting: We can browse by having at least one label having the specified ltree, with a query like” SELECT * FROM cecase WHERE mytag ~ *.emea.* For example, if we want all the records having at least a label for the .emea. (a branch named emea in any point in the hierarchical label): Don’t forget to check out the video linked above, as it demonstrates the demo working live as the data is being sent to the application! If you want to checkout the source, here is the code repo:   These advanced query capabilities offered by PostgreSQL can be used as a foundation to identify events due to reprocessing, manual inspection, triggering a workflow, etc. …but that is maybe subject for a second iteration on this demo… CONCLUSIONS This demo showcases the power of combining declarative logic, persistence and other technologies to process and label CloudEvents effectively! We defined our logic using a combination of expression and rules in the form of decision tables, combined with the use of PostgreSQL as a data store thanks to its advanced query capabilities, allowing for a more efficient and effective handling of the events. We hope you enjoyed our demo and look forward to hearing your feedback! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title type="html">How does Knative project compare to Dapr?</title><link rel="alternate" href="http://www.ofbizian.com/2023/02/how-does-knative-project-compare-to-dapr.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2023/02/how-does-knative-project-compare-to-dapr.html</id><updated>2023-02-08T10:30:00Z</updated><content type="html">HOW DOES KNATIVE PROJECT COMPARE TO DAPR? Both and projects help create and run cloud-native applications on Kubernetes, but differ in important aspects. I thought I'd quickly share where these projects overlap and complement each other from a user point of view. If you prefer, you can read this post as a twitter too. Dapr vs Knative TLDR: Knative extends Kubernetes with serverless containers (scaling to and from 0) and helps you connect applications declaratively. Dapr helps developers implement reliable connected distributed applications quickly.  COMMUNITY Knative originated from Google, whereas Dapr from Microsoft. Today both projects are incubating at CNCF. Both projects have growing communities and are within top 20 active CNCF projects (Dapr #10 and Knative #17) Community statistics See the full CNCF project statistics . PRIMARY FOCUS AREA ➤Knative extends Kubernetes with serverless containers by taking care of runtine networking (sync/async), autoscaling (to/from zero), and app revision tracking. ➤Dapr helps developers create reliable connected distributed applications quickly. It doesn’t manage the lifecycle of the application, instead it runs next to the applications. TARGET USER ➤Knative serving can be used by Ops to auto-scale and release apps (with traffic splitting). Knative eventing and functions can be used by devs to build, deploy apps, and connect external systems and event-driven containers.  ➤Dapr is toolkit designed primarily for developers. Developers use APIs &amp;amp; SDKs to interact with Dapr and offload responsibilities such as: pub/sub, state access, stateful service invocation, resiliency, etc. There are design time and runtime benefits for architects and operations teams respectively as described . SUPPORTED PLATFORMS ➤Knative runs only on top of Kubernetes and a network layer such as Kourier, Istio, Contour. ➤Dapr can run on Kubernetes, as well as on-premises and edge devices (such as the ). For local dev, Knative requires Kubernetes, whereas Dapr can also run on Docker, or as a single binary only. DEPLOYMENT MODEL Both projects have operator, helm chats, CLI that help with installation and operating the control planes on Kubernetes.  ➤On the data plane side, Dapr is a sidecar that gets injected into the application pod. The application interacts with Dapr over well-defined APIs... ➤Knative dictates how the application is defined and run on Kubernetes by creating deployments, pods, configmaps, and networking configurations. It injects a transparent sidecar into every pod to measure network activity. And has an activator to hold off requests while scaling from zero. DEVELOPER EXPERIENCE ➤Knative uses Kubernetes CRDs for defining an app (called Knative Service) composed of container, configuration, revision. It also offers CRDs that can define how events (CloudEvents) flow between these services, subscribe to a broker. And functions-centric CLI. ➤Dapr offers HTTP and gRPC APIs for Service invocation, Pub/Sub, State, Workflows, Bindings, Configuration, Secrets, Distributed lock, called building blocks. Devs use an HTTP/gRPC client, or SDKs for 8+ languages to interact with the above APIs. OPERATIONAL EXPERIENCE ➤Knative serving helps Ops with releasing, auto-scaling, configuring services. Knative eventing with abstracting the broker.  ➤Dapr helps Ops with monitoring, securing, and increasing resiliency of services, as well as cloud infrastructure abstraction. TOP FEATURES ➤Knative Scale to 0 Autoscaling Traffic splitting App definition Pub/sub Connectors Function CLI ➤Dapr Pub/sub Service-to-service interaction with resiliency, key/value access, Actors, Connectors, Security (mTLS, Auth), Config/Secrets, Workflows, Distributed Lock APIs. FEATURE OVERLAP The primary overlap is around pub/sub capabilities. Both projects offer async interactions between applications by abstracting the broker &amp;amp; using CloudEvents format. Both projects have connectors and ability to subscribe apps to the broker.  ➤Knative defines these w/ CRDs &amp;amp; over HTTP.  ➤Dapr supports HTTP/gRPC, using CRDs and code. SWEET SPOT ➤Knative: autoscaling containers (to and from zero). ➤Dapr: event-driven and stateful service interactions. Upcoming hot feature:  ➤Knative: function development.  ➤Dapr: workflows based orchestration. SUMMARY Those are the key differences between Dapr and Knative I'm aware of.  If you know other differences, share those on the twitter . We are actively working on both projects and exploring how they complement each other and integrate with the greater CNCF .</content><dc:creator>Unknown</dc:creator></entry><entry><title>How JBoss EAP 8-Beta makes deployment on OpenShift easier</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/08/how-jboss-eap-8-beta-makes-deployment-openshift-easier" /><author><name>Philip Hayes</name></author><id>64d51ce9-7c63-4ea8-a224-fbd3fe72cd98</id><updated>2023-02-08T07:00:00Z</updated><published>2023-02-08T07:00:00Z</published><summary type="html">&lt;p&gt;The recent &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html/release_notes_for_red_hat_jboss_enterprise_application_platform_8.0_beta/index?extIdCarryOver=true&amp;sc_cid=701f2000001Css5AAC"&gt;release of Red Hat JBoss EAP 8-Beta&lt;/a&gt; introduced changes to the provisioning and configuration of JBoss EAP application images on &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The process adopted the JBoss &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html-single/using_jboss_eap_on_openshift_container_platform/index#assembly_provisioning-a-jboss-eap-server-using-the-maven-plugin_assembly_environment-variables-and-model-expression-resolution"&gt;EAP Maven Plugin&lt;/a&gt;, which provides significant improvements making the configuration of JBoss EAP on OpenShift easier and more flexible.&lt;/p&gt; &lt;p&gt;This article demonstrates the steps required to add the JBoss EAP Maven Plugin to an existing JBoss EAP 8-Beta application. We will also test our plugin locally and then build and deploy our application to OpenShift using Helm via the OpenShift UI and the Helm CLI.&lt;/p&gt; &lt;h2&gt;The benefits of the JBoss EAP Maven plugin&lt;/h2&gt; &lt;p&gt;The JBoss EAP Maven plugin provides the following functionalities:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Uses the wildfly-ee-galleon-pack and &lt;a href="https://github.com/jbossas/eap-cloud-galleon-pack/blob/main/doc/index.md"&gt;eap-cloud-galleon-pack&lt;/a&gt; Galleon feature-packs and selection of layers for customizing the server configuration file.&lt;/li&gt; &lt;li aria-level="1"&gt;Applies CLI script commands to the server. These CLI scripts can be easily added to the application source code repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Supports the addition of extra files into the server installation, such as a keystore file.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html-single/using_jboss_eap_on_openshift_container_platform/index#assembly_provisioning-a-jboss-eap-server-using-the-maven-plugin_assembly_environment-variables-and-model-expression-resolution"&gt;documentation&lt;/a&gt; for more information. Refer to the &lt;a href="https://github.com/jboss-developer/jboss-eap-quickstarts/tree/8.0.x"&gt;quickstarts for JBoss EAP 8-Beta&lt;/a&gt; where you will find working examples for your project.&lt;/p&gt; &lt;p&gt;When building a JBoss EAP 7.x image for OpenShift, the OpenShift builder image provisioned the JBoss EAP server instance. The configuration was provided by a combination of runtime variables, configuration snippets, and JBoss CLI scripts.&lt;/p&gt; &lt;p&gt;With JBoss EAP 8-Beta, the JBoss EAP Maven plugin provisions the server and deploys the packaged application during the Maven execution. All the configuration for this build process is maintained in the Maven pom.xml file. This allows developers and operations teams to control and test their EAP deployments in their local environments, which provides significant benefits.&lt;/p&gt; &lt;p&gt;The JBoss EAP Maven plugin uses &lt;a href="https://github.com/wildfly/galleon#overview"&gt;Galleon&lt;/a&gt; to provision a JBoss EAP server configured with the minimum set of features to support the deployed application. Galleon is a provisioning tool for working with Maven repositories. Galleon automatically retrieves released JBoss EAP Maven artifacts to compose a software distribution of a JBoss EAP-based application server according to a user's configuration.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://helm.sh/docs/intro/install/"&gt;Helm CLI&lt;/a&gt;. Version 3.5+  (only required if you want to use Helm CLI instead of OpenShift Dev Console)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;OpenShift cluster&lt;/a&gt;. Version 4.11+&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://maven.apache.org/download.cgi"&gt;Maven&lt;/a&gt;. Version 3.8.5+&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Project setup&lt;/h2&gt; &lt;p&gt;We're going to start with a simple "hello world" JBoss EAP 8-Beta application. You can check out the source from the &lt;a href="https://github.com/deewhyweb/eap8-on-ocp"&gt;GitHub page&lt;/a&gt;. We can create the deployment artifact from this project by running the following command from the folder you cloned your code into.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our application artifact should now be available at:  &lt;strong&gt;./target/helloworld.war&lt;/strong&gt;. If you want to test this application in JBoss EAP 8-Beta, you can do so by following these instructions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Download the JBoss EAP 8-Beta distribution &lt;a href="https://developers.redhat.com/products/eap/download"&gt;here&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Extract the distribution into a local folder (e.g., ~/jboss-eap-8).&lt;/li&gt; &lt;li&gt;Set the EAP_HOME environment variable as follows:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;export EAP_HOME=~/jboss-eap-8&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Start JBoss EAP 8-Beta with the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;$EAP_HOME/bin/standalone.sh&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Connect to EAP using the CLI from the folder you cloned your code into by running the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;$EAP_HOME/bin/jboss-cli.sh --connect&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Deploy our application artifact as follows:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;[standalone@localhost:9990 /] deploy ./target/helloworld.war&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;You should now be able to access the sample application at:  &lt;a href="http://localhost:8080/helloworld/HelloWorld"&gt;http://localhost:8080/helloworld/HelloWorld&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Shut down the JBoss EAP 8-Beta server.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Adding OpenShift support&lt;/h2&gt; &lt;p&gt;To add OpenShift support, we're going to add the JBoss EAP Maven plugin to our project and create an OpenShift profile. To do this, add the following to the pom.xml file in the &lt;strong&gt;&lt;profiles&gt;&lt;/strong&gt; section:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;        &lt;profile&gt;             &lt;id&gt;openshift&lt;/id&gt;             &lt;build&gt;                 &lt;plugins&gt;                     &lt;plugin&gt;                         &lt;groupId&gt;org.jboss.eap.plugins&lt;/groupId&gt;                         &lt;artifactId&gt;eap-maven-plugin&lt;/artifactId&gt;                         &lt;version&gt;1.0.0.Beta6-redhat-00001&lt;/version&gt;                         &lt;configuration&gt;                             &lt;channels&gt;                                 &lt;channel&gt;                                     &lt;groupId&gt;org.jboss.eap.channels&lt;/groupId&gt;                                     &lt;artifactId&gt;eap-8.0-beta&lt;/artifactId&gt;                                 &lt;/channel&gt;                             &lt;/channels&gt;                             &lt;feature-packs&gt;                                 &lt;feature-pack&gt;                                     &lt;location&gt;org.jboss.eap:wildfly-ee-galleon-pack&lt;/location&gt;                                 &lt;/feature-pack&gt;                                 &lt;feature-pack&gt;                                     &lt;location&gt;org.jboss.eap.cloud:eap-cloud-galleon-pack&lt;/location&gt;                                 &lt;/feature-pack&gt;                             &lt;/feature-packs&gt;                             &lt;layers&gt;                                 &lt;layer&gt;cloud-server&lt;/layer&gt;                             &lt;/layers&gt;                             &lt;filename&gt;ROOT.war&lt;/filename&gt;                         &lt;/configuration&gt;                         &lt;executions&gt;                             &lt;execution&gt;                                 &lt;goals&gt;                                     &lt;goal&gt;package&lt;/goal&gt;                                 &lt;/goals&gt;                             &lt;/execution&gt;                         &lt;/executions&gt;                     &lt;/plugin&gt;                 &lt;/plugins&gt;             &lt;/build&gt;         &lt;/profile&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Introducing this profile to our maven configuration makes the following updates:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creates an "openshift" profile with a "package" goal.&lt;/li&gt; &lt;li aria-level="1"&gt;Adds the eap-maven-plugin.&lt;/li&gt; &lt;li aria-level="1"&gt;Configures the eap-maven-plugin with feature packs. Feature packs are ZIP archives that are normally deployed to artifact repositories (such as Maven), where they can be used by Galleon tools. These features are then defined in predefined configuration layers that can be used to create installation configurations. The use of layers from feature packs enables the deployment of an EAP instance, providing minimal features to support the deployed application. For this project, we're adding two feature packs: &lt;ul&gt;&lt;li aria-level="2"&gt;org.jboss.eap:wildfly-ee-galleon-pack &lt;ul&gt;&lt;li aria-level="3"&gt;The wildfly-ee-galleon-pack contains the features required to build an instance of JBoss EAP. This feature pack contains several layers (e.g., jaxrs-server and cloud-server).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;org.jboss.eap.cloud:eap-cloud-galleon-pack &lt;ul&gt;&lt;li aria-level="3"&gt;The org.jboss.eap.cloud:eap-cloud-galleon-pack Galleon feature-pack provisions a set of additional features allowing you to configure a JBoss EAP server to run on the cloud.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Inclusion of the cloud-server layer. The cloud-server layer is the minimum layer required for applications deployed on OpenShift. This layer provides subsystems such as health and metrics.&lt;/li&gt; &lt;li aria-level="1"&gt;Setting the filename to ROOT.war ensures the application is deployed in the root context.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Testing the JBoss EAP Maven plugin&lt;/h2&gt; &lt;p&gt;Now that we've added the OpenShift profile and eap-maven-plugin, we can test the provisioning of a JBoss EAP 8-Beta instance. To do this, we run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn package -Popenshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will perform these tasks:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create a &lt;strong&gt;/target/server&lt;/strong&gt; folder and deploy an instance of JBoss EAP. The JBoss EAP instance will only contain the functionality defined by the &lt;strong&gt;&lt;layers&gt;&lt;/strong&gt; section of the pom.xml file, in this case, the cloud-server layer.&lt;/li&gt; &lt;li aria-level="1"&gt;Package the project code and name the resulting artifact &lt;strong&gt;ROOT.war&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Start the JBoss EAP instance from the &lt;strong&gt;/target/server&lt;/strong&gt; folder and deploy the &lt;strong&gt;ROOT.war&lt;/strong&gt; artifact.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The output of the command should be as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[INFO] Deploying ROOT.war [disconnected /] embed-server --server-config=standalone.xml [standalone@embedded /] deploy  ROOT.war --name=ROOT.war --runtime-name=ROOT.war [standalone@embedded /] stop-embedded-server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can test the instance of JBoss EAP provisioned by the JBoss EAP Maven plugin by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./target/server/bin/standalone.sh&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will start JBoss EAP 8-Beta. Looking at the logs, you should see a couple of things of note:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0010: Deployed "ROOT.war" (runtime-name : "ROOT.war")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The packaged application has been deployed as ROOT.war. This will ensure the application is deployed to the root context.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0054: Admin console is not enabled&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The admin console is not enabled for this instance of JBoss EAP. This is the recommended approach for JBoss EAP applications deployed on OpenShift.&lt;/p&gt; &lt;p&gt;Navigate to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/local_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/local_1.jpg?itok=ALQoRl_h" width="600" height="417" alt="The Hello World application running on localhost." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The JBoss EAP Hello World application running on the localhost.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now that we are confident the eap-maven-plugin is configured correctly, we can move on to deploy the application to OpenShift.&lt;/p&gt; &lt;h2&gt;Deploying to OpenShift&lt;/h2&gt; &lt;p&gt;To deploy our application to OpenShift, we will utilize Helm charts using the OpenShift UI and the Helm CLI tool.&lt;/p&gt; &lt;p&gt;First, we need to create a Helm configuration by following these steps:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Create a folder called "charts" in the folder you cloned the eap8-on-ocp code into.&lt;/li&gt; &lt;li&gt;In this folder, create a file called "helm.yaml" with the following contents:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt;build:   uri: https://github.com/deewhyweb/eap8-on-ocp.git   ref: 8.0.x deploy:   replicas: 1&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;In this example, we're using a pre-prepared branch containing source code updated to include the eap-maven-plugin configuration. You can also use your repository by replacing the information in this yaml file.&lt;/li&gt; &lt;li&gt;You will need to push the changes to pom.xml described above for the build to execute correctly.&lt;/li&gt; &lt;li&gt;We're now ready to deploy our application to OpenShift.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Deploying the Helm chart using the OpenShift UI&lt;/h3&gt; &lt;p&gt;Follow these steps to deploy the Helm chart in the OpenShift UI:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Log in to OpenShift.&lt;/li&gt; &lt;li&gt;Open the Developer UI. &lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Helm&lt;/strong&gt; in the left-hand menu.&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install a Helm Chart from the developer catalog&lt;/strong&gt;. This will bring you to the &lt;strong&gt;Helm Charts&lt;/strong&gt; catalog page (Figure 2).&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Filter by keyword&lt;/strong&gt; field, enter:  "eap"&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/helm-catalog.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/helm-catalog.jpg?itok=ojh4PkxQ" width="600" height="236" alt="The Helm charts catalog in OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Helm charts catalog in OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;From the list of Helm charts, select &lt;strong&gt;eap8&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install Helm Chart&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Switch to the &lt;strong&gt;YAML view&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Paste the configuration we created in charts/helm.yaml, as shown in Figure 3.&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install&lt;/strong&gt; to deploy the Helm chart.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/install-helm-chart.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/install-helm-chart.jpg?itok=-DPRlPPe" width="600" height="408" alt="The screen to install EAP Helm chart." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The screen to install the EAP Helm chart on OpenShift from the Helm catalog.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h3&gt;Deploying the Helm chart using the Helm CLI&lt;/h3&gt; &lt;p&gt;Now we need to deploy the Helm chart. We will use &lt;span&gt; the Helm CLI. Follow these steps:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;span&gt;Use the jboss-eap/eap8 Helm chart with our configuration:&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;helm install helloworld -f charts/helm.yaml --repo https://jbossas.github.io/eap-charts/ eap8&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;When this command completes, you should see something like this:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;NAME: helloworld LAST DEPLOYED: Fri Dec 16 12:52:27 2022 NAMESPACE: eap8-helm STATUS: deployed REVISION: 1 TEST SUITE: None NOTES:&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Your EAP 8 application is building! To follow the build, run this command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;oc get build -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Note: Your deployment will report &lt;strong&gt;ErrImagePull&lt;/strong&gt; and &lt;strong&gt;ImagePullBackOff&lt;/strong&gt; until the build is complete. Once the build is complete, your image will be automatically rolled out.)&lt;/p&gt; &lt;ul&gt;&lt;li&gt;To follow the deployment of your application, run the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;oc get deployment helloworld -w&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Testing the application deployment&lt;/h3&gt; &lt;p&gt;In the OpenShift UI, you should see two builds run and complete, then a pod will be deployed containing your JBoss EAP instance and application deployed (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_0_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_0_0.jpg?itok=7YpNZ6Pf" width="600" height="375" alt="JBoss EAP 8-Beta application deployed on OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The JBoss EAP 8-Beta application deployed on OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To test your application, click on the route icon. Alternatively, to find the route with the OpenShift CLI, enter this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get routes&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see something this output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME         HOST/PORT                                                             PATH   SERVICES     PORT    TERMINATION     WILDCARD helloworld   helloworld-eap8-helm.apps.cluster-xxx.xxx.hostname.com          helloworld   &lt;all&gt;   edge/Redirect   None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate to this route with http:// + host in your browser.&lt;/p&gt; &lt;p&gt;You should see &lt;strong&gt;Hello World!&lt;/strong&gt; as with the local host instance we deployed earlier. Our application has now been successfully built and deployed to OpenShift.&lt;/p&gt; &lt;h2&gt;Recap&lt;/h2&gt; &lt;p&gt;JBoss EAP 8-Beta uses the eap-maven-plugin to provision and configure EAP for OpenShift images. In this article, we demonstrated how to add the eap-maven-plugin to an existing JBoss EAP 8-Beta application and the associated benefits. We then use this plugin to test the provisioning of a JBoss EAP 8-Beta server locally, and then deploy this application to OpenShift using the EAP8 Helm chart.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/08/how-jboss-eap-8-beta-makes-deployment-openshift-easier" title="How JBoss EAP 8-Beta makes deployment on OpenShift easier"&gt;How JBoss EAP 8-Beta makes deployment on OpenShift easier&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Philip Hayes</dc:creator><dc:date>2023-02-08T07:00:00Z</dc:date></entry></feed>
